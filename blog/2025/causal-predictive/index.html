<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Prediction: Why Causality Matters | Nikolas Kougioulis </title> <meta name="author" content="Nikolas Kougioulis"> <meta name="description" content="And what your ML models are missing"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?6cfe3450948a4590503d2a9cdf8e92ed"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?3a2c9593790c8dcbf71db3e9feed3ce2"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nkougioulis.com"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Beyond Prediction: Why Causality Matters",
            "description": "And what your ML models are missing",
            "published": "December 31, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Nikolas</span> Kougioulis </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications &amp; Manuscripts </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Beyond Prediction: Why Causality Matters</h1> <p>And what your ML models are missing</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#why-causation-matters">Why Causation Matters</a> </div> <div> <a href="#the-common-cause-principle">The Common Cause Principle</a> </div> <div> <a href="#the-language-of-causality-scms-pearl-s-ladder">## The Language of Causality - SCMs &amp; Pearl's Ladder</a> </div> <div> <a href="#how-do-we-discover-causal-graphs">How Do We Discover Causal Graphs</a> </div> <div> <a href="#an-illustrative-example">An Illustrative Example</a> </div> <div> <a href="#closing-thoughts">Closing Thoughts</a> </div> </nav> </d-contents> <h3 id="introduction-">Introduction ü•öüê•</h3> <p>Modern data systems, from recommendation engines to climate analytics, are built almost entirely on <em>predictive modeling</em>. In a nutshell, one collects a vast number of observational samples (that are created under variable experimental scenarios), fits increasingly complex models (from simple regression models to fancy deep neural networks), and optimize for accuracy. Pretty much, classification models are fitted on target variables of interest, and decision policies are taken simply using these models. If $f$ is such a fitted classification model and $X$ a vector of features (e.g. cost of acquiring a customer and manufacturing expenses), then one may fit a predictive model on a variable of interest $y$ (e.g. expected sales) by $y = f(X)$ and then use it for sales prediction by plugging in values of $X$. Beneath the surface however lies a <b>fundamental limitation</b> of predictive models, which becomes crucial whenever we want to answer questions such as:</p> <ul> <li> <p>‚ùì What will happen if I set the price of a product to, let‚Äôs say, $5$ euros? Will sales increase?</p> </li> <li> <p>‚ùì Why did an outcome occur?</p> </li> <li> <p>‚ùì How do we make decisions that remain valid when conditions change (e.g. we test a drug on mice and are interested whether its effect changes under a distribution shift, e.g. on a population of humans)</p> </li> </ul> <p>Overall, predictive models excel at recognizing associations. Causal models on the other hand represent the underlying mechanisms of data. The difference between the two, although subtle at first, defines the boundary between pattern recognition and scientific reasoning. As Scholkopf et al. <d-cite key="scholkopf2021towards"></d-cite> point out:</p> <blockquote> If we wish to incorporate learning algorithms into human decision making, we need to trust that the predictions of the algorithm will remain valid if the experimental conditions are changed. </blockquote> <p>In this post, we‚Äôll walk through why causal models matter, how causal reasoning differs from prediction and illustrate the stakes, along with a minimal example. It does not serve as a complete treatment of causality, but as a motivational introduction to the unfamiliar reader. For a thorough treatment, we refer the interested reader to <d-cite key="pearl2009causality"></d-cite>, <d-cite key="spirtes2001causation"></d-cite>, and <d-cite key="pearl2018book"></d-cite>. Chapter 1 of <d-cite key="kougioulis2025large"></d-cite> serves as a detailed version of this blog post.</p> <h2 id="why-causality-matters">Why Causality Matters</h2> <p>Predictive models rely on observed <em>correlations</em> and <em>patterns</em> in observational data (i.e. samples that are purely observed, not obtained under a specific manipulation of the examined system). They implicitly assume that all samples come from a <b>single, stable distribution</b> (the familiar i.i.d. assumption). Under this assumption, identifying strong associations can be enough to make good and powerful predictions.</p> <p>However, associations alone cannot tell us <b>what causes what</b>. Consider the following, basic example:</p> <h3 id="predicting-ice-cream-sales-">Predicting Ice Cream Sales üç¶</h3> <p>It is known that during the summer, ice cream sales are increased compared to other seasons, with number of sunburn cases also showing an higher trend. We know (taken as expert knowledge) that although these two quantities are correlated, ice cream sales do not cause sunburn cases and vice-versa. Instead, these two quantities share a <em>confounder (common cause)</em> like the sun‚Äôs radiation, or even temperature. In any case, even if they have more than one confounding variable, we are allowed to treat them both as a single. The fact that $\text{ice cream sales} \leftarrow sun \rightarrow \text{sunburn}$, highlights our previous discussions, where a directed arrow represents a direct causal relationship (from a cause to its direct effect(s)).</p> <p>Now imagine an alternate world where everyone wears suncreen üß¥ (we intervened on sun‚Äôs radiation indirectly, by forcing lower radiation by sunscreen use): Sunburn cases plummet, but ice cream sales remain unchanged.Will ice cream sales increase, decrease or remain unaffected (similarly for sunburn cases)?</p> <p>A predictive model would infer: <b>more sunburn ‚Üí more ice cream sales</b>. We know this is wrong, but the model doesn‚Äôt. Both variables are effects of a <i>third, hidden cause:</i> <b>sunlight intensity</b>. This unobserved confounder creates misleading correlations, and as a result, any predictive model trained on the original correlation will <b>catastrophically fail</b>, and any inferred decisions cannot be taken seriously.</p> <p>This example captures the core limitation of predictive modeling:</p> <blockquote> üí° Predictive patterns break when the observed environment changes, while causal mechanisms do not. </blockquote> <p>The following table briefly shows scenarios where predictive and causal queries differ, and as a result, how predictive models can lead to false interpretations.</p> <table> <thead> <tr> <th><strong>Task</strong></th> <th><strong>Query</strong></th> <th><strong>Example</strong></th> <th><strong>Description</strong></th> <th><strong>Causal Model</strong></th> <th><strong>Predictive Model</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Prediction</strong></td> <td>Predict / diagnose <strong>$Y$ given $X$</strong> </td> <td>‚ÄúWhat is $Y$ when $X_1$ = 5?‚Äù</td> <td>Standard supervised prediction</td> <td>‚úîÔ∏è Correct predictions</td> <td>‚úîÔ∏è Correct predictions</td> </tr> <tr> <td><strong>Decision Making</strong></td> <td>Optimal $X$ to increase $Y$ under constraints</td> <td>‚ÄúWhat $X_1$ maximizes Y given $X_2 = 6$?‚Äù</td> <td>Choosing an action that changes the system</td> <td>‚úîÔ∏è Correct decisions</td> <td>‚ùå Possibly wrong decisions</td> </tr> <tr> <td><strong>What-if</strong></td> <td>Hypothetical changes (interventions)</td> <td>‚ÄúWhat if I set $X_1 = 5$?‚Äù</td> <td>Interventional reasoning, requires <em>$do(X)$</em> </td> <td>‚úîÔ∏è Correct estimate</td> <td>‚ùå Possibly wrong estimate</td> </tr> <tr> <td><strong>Interpretation</strong></td> <td>Feature importance / effect of $X$ on $Y$</td> <td>‚ÄúDoes $X_1$ affect $Y$?‚Äù</td> <td>Understanding influence of features</td> <td>‚úîÔ∏è Correct estimate</td> <td>‚ùå SHAP/feature importance may be misleading</td> </tr> <tr> <td><strong>Counterfactual</strong></td> <td>‚ÄúWhat would $Y$ have been if $X$ had been different?‚Äù</td> <td>‚Äú$Y=3$ when $X_3=$yellow. What if $X_3$=green?‚Äù</td> <td>Individual-level alternative-world reasoning</td> <td>‚úîÔ∏è Correct estimate</td> <td>‚ùå Generally impossible</td> </tr> <tr> <td><strong>Root Cause</strong></td> <td>Identify cause of an event</td> <td>‚ÄúWhat caused the failure?‚Äù</td> <td>Find initial causal driver</td> <td>‚úîÔ∏è Correct estimate</td> <td>‚ùå Possibly wrong</td> </tr> </tbody> </table> <h2 id="the-common-cause-principle">The Common Cause Principle</h2> <p>But how can observational data mislead us? Reichenbach‚Äôs <b>common cause principle</b><d-cite key="reichenbach1956direction"></d-cite> states:</p> <blockquote> <p>üí° If X and Y are correlated, then either X causes Y, Y causes X, or a hidden confounder causes both.</p> </blockquote> <p>Observational data cannot tell these apart. This limitation explains famous phenomena like <b>Simpson‚Äôs paradox</b><d-cite key="simpson1951interpretation"></d-cite>, where aggregated correlations reverse once you account for confounders. It also explains why <b>SHAP values and feature importance</b>, though useful, are not causal measures. They reflect importance <b>within the model</b>, not influence <b>in the real world</b>.</p> <h2 id="randomized-experiments-the-gold-standard-for-causality">Randomized Experiments: The Gold Standard for Causality</h2> <p>Given the limitations of observational data, a natural question arises: <i>how can causal effects be measured correctly?</i> Since the seminal work of Ronald Fisher&lt;d-cite key‚Äùfisher1935design‚Äù /&gt;, the gold standard for causal inference has been <b>randomized experimentation</b>, and in particular <b>Randomized Controlled Trials (RCTs)</b>.</p> <p>Randomized experiments aim to isolate causal effects by deliberately intervening on one or more variables of interest while holding all other factors constant <b>in expectation</b>. This is typically achieved through <b>random assignment</b>, which ensures that both observed and unobserved covariates are, on average, balanced across experimental groups. As a result, randomization removes confounding <i>by design</i>, allowing causal effects to be identified without relying on strong modeling assumptions.</p> <div class="row justify-content-sm-center"> <div class="col-sm-auto text-center mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ab-testing-480.webp 480w,/assets/img/ab-testing-800.webp 800w,/assets/img/ab-testing-1200.webp 1200w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ab-testing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption mt-2" style="text-align: center;">An A/B test on an online platforms is an instance of a randomized controlled trial (RCT). Users are randomly assigned to interact with one of <i>two</i> webpage versions, for example, a new interface (left) or the existing <i>control</i> version (right). Outcomes of interest (e.g., user engagement) are measured over a fixed time period. Statistical analysis is then used to determine whether the introduced interface has a statistically significant causal effect on the outcome variables (e.g., exposure rates or sales). Illustration courtesy of Abstraktmg<d-footnote>https://www.abstraktmg.com/a-b-testing-in-marketing/ (last accessed: 30 December 2025).</d-footnote>. </div> </div> </div> <p>Common instances of randomized experimentation include:</p> <ul> <li> <b>Randomized Controlled Trials (RCTs)</b> in medicine and the social sciences,</li> <li> <b>A/B tests</b> in online platforms and digital systems,</li> <li> <b>Controlled laboratory experiments</b> in the natural sciences,</li> <li> <b>Simulation-based interventions</b> in synthetic or virtual environments.</li> </ul> <p>To illustrate, consider a clinical trial investigating whether a newly introduced treatment (let‚Äôs call it $X$) reduces the severity of migraines. An RCT proceeds by randomly assigning patients to either a <b>treatment group</b> (receiving treatment X) or a <b>control group</b> (receiving a placebo), and subsequently comparing outcomes across the two groups. Because assignment is random, any systematic difference in outcomes can be attributed to the intervention itself rather than to pre-existing differences among patients.</p> <p>A standard causal quantity estimated in such settings is the <b>Average Treatment Effect (ATE)</b>, defined as $\text{ATE} = \mathbb{E}[Y_1 - Y_0]$ where $Y_1$ and $Y_0$ denote the potential outcomes under treatment and control, respectively. If the ATE is statistically distinguishable from zero, one concludes that the intervention has a causal effect on the outcome of interest.</p> <p>The conceptual strength of RCTs lies in their <b>ability to eliminate confounding bias through randomization</b>, making them the most reliable tool for causal inference. However, despite their methodological appeal, randomized experiments are often <b>impractical or infeasible</b> in real-world settings. They can be prohibitively expensive, logistically complex, or ethically unacceptable, for instance, when studying the causal effects of harmful behaviors such as smoking. Moreover, in modern large-scale systems, the space of possible interventions is often combinatorial, rendering exhaustive experimentation impossible in practice.</p> <p>These limitations motivate the development of <b>model-based causal frameworks</b> that allow causal effects to be inferred without relying exclusively on randomized experiments. In the following sections, we introduce such a framework through structural causal models, which enable principled reasoning about interventions, counterfactuals, and distributional changes beyond purely observational data.</p> <h2 id="the-language-of-causality---scms--pearls-ladder">The Language of Causality - SCMs &amp; Pearl‚Äôs Ladder</h2> <p>The formal language in Causality to reason about causal mechanisms, interventions and counterfactuals are <b>Structural causal models (SCMs)</b>, introduced by Judea Pearl <d-cite key="pearl2009causality"></d-cite>. An SCM consists of:</p> <ol> <li> <b>A Directed Acyclic Graph (DAG)</b> where nodes represent the examined variables of interest and direct arrows <b>direct causal effects</b> (e.g., Sun ‚Üí IceCream).</li> <li> <b>Structural equations</b> ($X_i = f_i(\mathrm{Parents}(X_i), \epsilon_{X_i})$), representing how each variable $X_i$ is generated.</li> <li> <b>Independent noise terms</b> $\epsilon_{X_i}$ for each variable $X_i$, representing unobserved and inherent randomness of the system.</li> </ol> <p>This framework allows us to compute three fundamentally different types of queries:</p> <table> <thead> <tr> <th>Type</th> <th>Question</th> <th>Example</th> </tr> </thead> <tbody> <tr> <td><b>Observational</b></td> <td>What do we see?</td> <td>$(P(Y \mid X=x))$</td> </tr> <tr> <td><b>Interventional</b></td> <td>What happens if we <i>force</i> X to a value?</td> <td>$(P(Y \mid do(X=x)))$</td> </tr> <tr> <td><b>Counterfactual</b></td> <td>What would have happened <i>otherwise</i>?</td> <td>$(P(Y_{x‚Äô} \mid X=x,Y=y))$</td> </tr> </tbody> </table> <aside><p>These three different types of queries are known as <b>Pearl's Ladder of Causal Hierarchy</b> <d-cite key="pearl2018book"></d-cite>, where predictive models can only answer queries of level 0, with causal models additionally to level 1 and 2.</p> </aside> <p>Predictive models only answer the first type, while causality accounts for all three.</p> <p>SCMs allow us to simulate interventions mathematically without performing physical experiments. Using <b>do-calculus</b>, we can compute $P(Y \mid do(X=x))$ which differs from the purely observational $P(Y \mid X=x)$ except in special, unconfounded cases (outside the scope of this blog). This gives us a way to predict interventions, <i>if we know the causal graph</i>.</p> <h2 id="how-do-we-discover-causal-graphs">How Do We Discover Causal Graphs?</h2> <p>This is where the field of <b>causal discovery</b> enters. Causal discovery attempts to learn the structure of the system (the DAG) from data (often observational, sometimes including interventional samples), which operate under certain causal assumptions. For example, the causal sufficiency assumption assumes no hidden confounders, while faithfulness (loosely defined in the scope of this blog post) that no determinism exists in the examined system, hence no causality (e.g. variables that are deterministically related like ratios of variables). Once the graph is known, <b>causal inference</b> methods estimate effects such as:</p> <ul> <li>ATEs</li> <li>mediation effects</li> <li>optimal intervention strategies</li> <li>counterfactual outcomes</li> </ul> <p>In practice, as the ground truth causal model is unknown, one first discovers the causal structure (the causal DAG and the SCM representation) and then estimates causal effects.</p> <h2 id="an-illustrative-example-the-provided-examples-are-available-in-a-curated-github-repositoryhttpsgithubcomkougiouliscausality-tutorial-with-various-causality-tutorials-for-the-motivated-reader">An Illustrative Example <d-footnote>The provided examples are available in a curated [Github Repository](https://github.com/kougioulis/causality-tutorial) with various causality tutorials for the motivated reader.</d-footnote> </h2> <p>Consider 3 variables, $X,Y,Z$. $X$ and $Z$ are observed to have correlation coefficient $3$. That is, if you observe $1$ unit of change in $X$, you observe $3$ units of change in $Z$. Let‚Äôs try to answer the following questions:</p> <aside> <p>For simplicity purposes, we assume linear relationships.</p> </aside> <ul> <li>‚ùì If you intervene and change $X$ (not merely observe) in the real world, what change would you observe?</li> <li>‚ùì If you build a predictive model from $X$ to $Z$ and change the input to the model, would you observe the same change when you intervene in the real world?</li> </ul> <p>For linear correlations and causal relations with Gaussian additive noise, we make the following assumptions:</p> <ul> <li>‚òùÔ∏è Correlations on a path <b>multiply</b> together</li> <li>‚òùÔ∏è Correlations from different path <b>sum</b> </li> </ul> <p>We begin by importing some needed modules:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div> <p>Consider a <b>causal</b> DAG, assuming linear relationships with edges $X \leftarrow Y \rightarrow Z$ and $X \rightarrow Z$. A directed edge illustrates direct causal influence (i.e. $X$ directly causes $Y$, $X$ is the direct cause of $Y$, $Y$ is the direct effect of $X$) while indirect causal relationships are illustrated by directed paths. The linear coefficients of each causal edge are $0.7$ for $Y \rightarrow X$, $-8$ for $Y \rightarrow Z$ and $13$ for $X \rightarrow Z$. Loosely defined, pairing this causal DAG with the functional dependencies of its variable given its parents (direct causes), plus an additive noise term (accounting for randomness), creates an (additive) structural causal model (SCM). Let‚Äôs try to answer the following question:</p> <blockquote>What is going to happen when I increase X by 1?</blockquote> <ul> <li>The non-causal path $X \leftarrow Y \rightarrow Z$ has coefficient $0.7 \cdot (-8)=-5.6$.</li> <li>The path $X -&gt; Z$ coefficient $13$ causal effect causal path.</li> <li>The total correlation coefficient observed is $0.7 - 8 = -7.3$.</li> </ul> <p>We observe that the <em>causal effect is larger than the computed correlation!</em> (Causal effect is positive, observed correlation is negative)</p> <p>We define the SCM of the three variables:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Defining structural equations of the SCM
# X &lt;- Y (coef 5) + noise
# Z &lt;- Y (coef -2) + X (coef 13) + noise
</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">6</span>  <span class="c1"># large enough sample size
</span>
<span class="c1"># Exogenous Gaussian noise
</span><span class="n">eps_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">eps_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">eps_z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

<span class="c1"># Structural Causal Model
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">eps_y</span>
<span class="n">X</span> <span class="o">=</span> <span class="mf">0.7</span> <span class="o">*</span> <span class="n">Y</span> <span class="o">+</span> <span class="n">eps_x</span>
<span class="n">Z</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span> <span class="o">*</span> <span class="n">Y</span> <span class="o">+</span> <span class="mi">13</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">eps_z</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y</span><span class="p">,</span> <span class="sh">"</span><span class="s">Z</span><span class="sh">"</span><span class="p">:</span> <span class="n">Z</span><span class="p">})</span>
</code></pre></div></div> <p>And compute the observed Pearson correlation between X and Z, as well as the predictive effect (regression $Z \sim X$):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pearson Correlation between X and Z (observational / predictive)
</span><span class="n">corr_xz</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">corrcoef</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Predictive model: Z ~ X by ordinary least squares regression
</span><span class="n">X_df</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nf">add_constant</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">])</span>
<span class="n">model_pred</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nc">OLS</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">Z</span><span class="sh">"</span><span class="p">],</span> <span class="n">X_df</span><span class="p">).</span><span class="nf">fit</span><span class="p">()</span>
<span class="n">pred_effect</span> <span class="o">=</span> <span class="n">model_pred</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Observed correlation between X and Z:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">corr_xz</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="c1"># 0.862
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Predictive effect (regression Z ~ X):</span><span class="sh">"</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">pred_effect</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="c1"># 9.246
</span></code></pre></div></div> <p>From the above snippet, we obtain the following contradictory results:</p> <ul> <li> <p><b>Causal model:</b> Increase $X$ by $1$, increase Z by $0.86$.</p> </li> <li> <p><b>Predictive model:</b> Increase $X$ by $1$, increase Z by $9.25$.</p> </li> </ul> <p>The correct approach, would instead be to:</p> <ul> <li>1Ô∏è‚É£ Learn the causal model (via a <b>causal discovery</b> algorithm, in this example we assume it is known correctly.)</li> <li>2Ô∏è‚É£ Identify the <b>non-causal paths</b> and remove the effect of the non-causal paths and only (identify the quantities that block the correlations from non-causal paths, called the <b>adjustment set</b>).</li> </ul> <aside> <p><b>Adjustment set:</b>a set of variables that, when conditioned on, blocks all backdoor (confounding) paths between a treatment and an outcome, enabling unbiased causal effect estimation from observational data.</p> </aside> <ul> <li>3Ô∏è‚É£ Build a <b>predictive model</b> that <b>includes an adjustment set</b> and only and hence controls for their values.</li> </ul> <p>The correlation coefficient of $X$ to $Z$ conditioned on fixed values of $Y$ provide the <b>true causal effect</b> of $13$ units with the following code snippet, with an absolute error in the third decimal place.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># True causal effect (by an intervention do(X))
</span><span class="n">causal_effect</span> <span class="o">=</span> <span class="mi">13</span>  <span class="c1"># known from our structural equations
</span>
<span class="c1"># Adjustment set approach (control for Y in regression)
</span><span class="n">XZ_df</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nf">add_constant</span><span class="p">(</span><span class="n">data</span><span class="p">[[</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">]])</span>
<span class="n">model_adj</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nc">OLS</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">Z</span><span class="sh">"</span><span class="p">],</span> <span class="n">XZ_df</span><span class="p">).</span><span class="nf">fit</span><span class="p">()</span>
<span class="n">adj_effect</span> <span class="o">=</span> <span class="n">model_adj</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Causal effect (true do-intervention): </span><span class="si">{</span><span class="n">causal_effect</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># 13
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Causal effect via adjustment set (regression Z ~ X + Y): </span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="n">adj_effect</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># 13.002
</span></code></pre></div></div> <div class="row justify-content-sm-center"> <div class="col-sm-auto text-center mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/interv_obs-480.webp 480w,/assets/img/interv_obs-800.webp 800w,/assets/img/interv_obs-1200.webp 1200w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/interv_obs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption mt-2" style="text-align: center;">Histogram of the conditional observational distribution $Z|X=1$ against the interventional distribution $Z|\text{do}(X=1)$ further illustrates the difference between the two (and convince us that in general, $P(Z | X=x) \neq P(Z | do(X=x))$).</div> </div> </div> <p>As shown in the histogram, the two distributions $Z \mid X \approx 1$ and $Z \mid do(X = 1)$ differ dramatically because:</p> <ul> <li> <p>Under <b>conditioning</b>, when we observe $X = 1$, that typically means <b>Y was high</b>, because $X = 0.7Y + \text{noise}$. Thus the backdoor path $X \leftarrow Y \rightarrow Z$ pushes Z down via the $-8Y$ term.</p> </li> <li> <p>Under <b>intervention</b>, when we force $X = 1$, $Y$ is no longer correlated, so only the direct causal effect $(13X)$ applies.</p> </li> </ul> <h2 id="back-to-the-ice-cream-sales-example">Back to the Ice Cream Sales Example</h2> <p>Let‚Äôs return to the ice cream example illustrated at the beginning of the post, but this time with some code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">samples</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># observational samples (iid)
</span>
<span class="c1"># True causal mechanism: ice_cream sales &lt;- sun -&gt; sunburn
</span><span class="n">sun</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="p">)</span>
<span class="n">ice_cream</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sun</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="p">)</span>
<span class="n">sunburn</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">sun</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Predictive model incorrectly learns sunburn -&gt; ice cream
</span><span class="n">X</span> <span class="o">=</span> <span class="n">sunburn</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">pred_model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ice_cream</span><span class="p">)</span>

<span class="c1"># Intervene: sunscreen campaign sets sunburn to a constant
</span><span class="n">sunburn_intervened</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Predictive model's WRONG prediction under intervention
</span><span class="n">predicted_icecream</span> <span class="o">=</span> <span class="n">pred_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">sunburn_intervened</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># True causal outcome under intervention - ice cream depends ONLY on sunlight, not sunburn (graph surgery is performed on the underlying SCM)
</span><span class="n">true_icecream</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sun</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">True Ice Cream Sales (causal model): </span><span class="si">{</span><span class="n">true_icecream</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> units</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># 10.04 units
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Predicted Ice Cream Sales (predictive model): </span><span class="si">{</span><span class="n">predicted_icecream</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> units</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># 2.05 units 
</span></code></pre></div></div> <p>A predictive model wrongly suggests that an increase in one unit of radiation will account for two units of increase in ice cream sales, which is 5 times less that the true causal effect which we obtained using the true causal model.</p> <p>As we have noticed already, a predictive model is doomed to failure if applied to cases outside the trained distribution: For example, consider applying the above simple regression model on a hypothetical subpopulation of Scandinavians (who are less exposed to sunlight compared to other regions) who enjoy eating ice cream all year long.</p> <h2 id="closing-thoughts">Closing Thoughts</h2> <p>Predictive models excel at finding patterns in data, but patterns alone are not enough when decisions, interventions, or changing environments are involved. Predictive models (either from classical ML up to deep learning approaches) have found great sucess, yet they prove unstable when the underlying perturbed.</p> <div style="max-width: 450px; margin: 0 auto; zoomable:true"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/causality_meme-480.webp 480w,/assets/img/causality_meme-800.webp 800w,/assets/img/causality_meme-1200.webp 1200w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/causality_meme.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Causal modeling addresses this gap by explicitly representing <b>how</b> data is generated. By reasoning about interventions and counterfactuals, causal methods allow models to generalize beyond the conditions under which they were trained and to support meaningful actions.</p> <p>It is to no surpise that decades after Pearl‚Äôs formulation of causality, the industry is just starting to adopt causal discovery and causal inference methods for optimized decision making and creation of <i>causal digital twins</i>, especially in the case of time-series data (by Temporal Structural Equation Models - TSCMs<d-cite key="runge2018causal"></d-cite><d-cite key="runge2019inferring"></d-cite>), now called <b>Causal AI</b> to contrast approaches using traditional ML or deep learning and LLMs.</p> <aside><p> A causal digital twin is a causal model of an examined system that combines a real-time state representation alongside a causal model (e.g., a structural causal model), allowing counterfactual reasoning, intervention analysis, and policy evaluation. </p></aside> <p>What‚Äôs your view? Let me know in the comments! üöÄ</p> <br> <hr> <br> If you found this post useful, please cite this as: <blockquote> <p>Kougioulis, Nikolas (Dec 2025). Beyond Prediction: Why Causality Matters. https://nkougioulis.com.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">kougioulis2025beyond-prediction-why-causality-matters</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Beyond Prediction: Why Causality Matters}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Kougioulis, Nikolas}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Dec}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://nkougioulis.com/blog/2025/causal-predictive/}</span>
<span class="p">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-12-31.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 880px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'kougioulis/kougioulis.github.io',
        'data-repo-id': 'R_kgDONjWI1w',
        'data-category': 'Comments',
        'data-category-id': 'DIC_kwDONjWI184CoYp0',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <div class="text-center"> <img src="/assets/img/yfjfd.png" alt="Watercolored phoenix drawing" class="img-fluid" style="max-width: 100%; height: auto;"> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0FGKYMMNXZ"></script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> </body> </html>