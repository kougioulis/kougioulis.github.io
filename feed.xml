<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://nkougioulis.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nkougioulis.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-17T16:41:40+00:00</updated><id>https://nkougioulis.com/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Beyond Prediction: Why Causality Matters</title><link href="https://nkougioulis.com/blog/2025/causal-predictive/" rel="alternate" type="text/html" title="Beyond Prediction: Why Causality Matters"/><published>2025-12-31T08:00:00+00:00</published><updated>2025-12-31T08:00:00+00:00</updated><id>https://nkougioulis.com/blog/2025/causal-predictive</id><content type="html" xml:base="https://nkougioulis.com/blog/2025/causal-predictive/"><![CDATA[<h3 id="introduction-">Introduction ü•öüê•</h3> <p>Modern data systems, from recommendation engines to climate analytics, are built almost entirely on <em>predictive modeling</em>. In a nutshell, one collects a vast number of observational samples (that are created under variable experimental scenarios), fits increasingly complex models (from simple regression models to fancy deep neural networks), and optimize for accuracy. Pretty much, classification models are fitted on target variables of interest, and decision policies are taken simply using these models. If $f$ is such a fitted classification model and $X$ a vector of features (e.g. cost of acquiring a customer and manufacturing expenses), then one may fit a predictive model on a variable of interest $y$ (e.g. expected sales) by $y = f(X)$ and then use it for sales prediction by plugging in values of $X$. Beneath the surface however lies a <b>fundamental limitation</b> of predictive models, which becomes crucial whenever we want to answer questions such as:</p> <ul> <li> <p>‚ùì What will happen if I set the price of a product to, let‚Äôs say, $5$ euros? Will sales increase?</p> </li> <li> <p>‚ùì Why did an outcome occur?</p> </li> <li> <p>‚ùì How do we make decisions that remain valid when conditions change (e.g. we test a drug on mice and are interested whether its effect changes under a distribution shift, e.g. on a population of humans)</p> </li> </ul> <p>Overall, predictive models excel at recognizing associations. Causal models on the other hand represent the underlying mechanisms of data. The difference between the two, although subtle at first, defines the boundary between pattern recognition and scientific reasoning. As Scholkopf et al. <d-cite key="scholkopf2021towards"></d-cite> point out:</p> <blockquote> If we wish to incorporate learning algorithms into human decision making, we need to trust that the predictions of the algorithm will remain valid if the experimental conditions are changed. </blockquote> <p>In this post, we‚Äôll walk through why causal models matter, how causal reasoning differs from prediction and illustrate the stakes, along with a minimal example. It does not serve as a complete treatment of causality, but as a motivational introduction to the unfamiliar reader. For a thorough treatment, we refer the interested reader to <d-cite key="pearl2009causality"></d-cite>, <d-cite key="spirtes2001causation"></d-cite>, and <d-cite key="pearl2018book"></d-cite>. Chapter 1 of <d-cite key="kougioulis2025large"></d-cite> serves as a detailed version of this blog post.</p> <h2 id="why-causality-matters">Why Causality Matters</h2> <p>Predictive models rely on observed <em>correlations</em> and <em>patterns</em> in observational data (i.e. samples that are purely observed, not obtained under a specific manipulation of the examined system). They implicitly assume that all samples come from a <b>single, stable distribution</b> (the familiar i.i.d. assumption). Under this assumption, identifying strong associations can be enough to make good and powerful predictions.</p> <p>However, associations alone cannot tell us <b>what causes what</b>. Consider the following, basic example:</p> <h3 id="predicting-ice-cream-sales-">Predicting Ice Cream Sales üç¶</h3> <p>It is known that during the summer, ice cream sales are increased compared to other seasons, with number of sunburn cases also showing an higher trend. We know (taken as expert knowledge) that although these two quantities are correlated, ice cream sales do not cause sunburn cases and vice-versa. Instead, these two quantities share a <em>confounder (common cause)</em> like the sun‚Äôs radiation, or even temperature. In any case, even if they have more than one confounding variable, we are allowed to treat them both as a single. The fact that $\text{ice cream sales} \leftarrow sun \rightarrow \text{sunburn}$, highlights our previous discussions, where a directed arrow represents a direct causal relationship (from a cause to its direct effect(s)).</p> <p>Now imagine an alternate world where everyone wears suncreen üß¥ (we intervened on sun‚Äôs radiation indirectly, by forcing lower radiation by sunscreen use): Sunburn cases plummet, but ice cream sales remain unchanged.Will ice cream sales increase, decrease or remain unaffected (similarly for sunburn cases)?</p> <p>A predictive model would infer: <b>more sunburn ‚Üí more ice cream sales</b>. We know this is wrong, but the model doesn‚Äôt. Both variables are effects of a <i>third, hidden cause:</i> <b>sunlight intensity</b>. This unobserved confounder creates misleading correlations, and as a result, any predictive model trained on the original correlation will <b>catastrophically fail</b>, and any inferred decisions cannot be taken seriously.</p> <p>This example captures the core limitation of predictive modeling:</p> <blockquote> üí° Predictive patterns break when the observed environment changes, while causal mechanisms do not. </blockquote> <p>The following table briefly shows scenarios where predictive and causal queries differ, and as a result, how predictive models can lead to false interpretations.</p> <table> <thead> <tr> <th><strong>Task</strong></th> <th><strong>Query</strong></th> <th><strong>Example</strong></th> <th><strong>Description</strong></th> <th><strong>Causal Model</strong></th> <th><strong>Predictive Model</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Prediction</strong></td> <td>Predict / diagnose <strong>$Y$ given $X$</strong></td> <td>‚ÄúWhat is $Y$ when $X_1$ = 5?‚Äù</td> <td>Standard supervised prediction</td> <td>‚úîÔ∏è Correct predictions</td> <td>‚úîÔ∏è Correct predictions</td> </tr> <tr> <td><strong>Decision Making</strong></td> <td>Optimal $X$ to increase $Y$ under constraints</td> <td>‚ÄúWhat $X_1$ maximizes Y given $X_2 = 6$?‚Äù</td> <td>Choosing an action that changes the system</td> <td>‚úîÔ∏è Correct decisions</td> <td>‚ùå Possibly wrong decisions</td> </tr> <tr> <td><strong>What-if</strong></td> <td>Hypothetical changes (interventions)</td> <td>‚ÄúWhat if I set $X_1 = 5$?‚Äù</td> <td>Interventional reasoning, requires <em>$do(X)$</em></td> <td>‚úîÔ∏è Correct estimate</td> <td>‚ùå Possibly wrong estimate</td> </tr> <tr> <td><strong>Interpretation</strong></td> <td>Feature importance / effect of $X$ on $Y$</td> <td>‚ÄúDoes $X_1$ affect $Y$?‚Äù</td> <td>Understanding influence of features</td> <td>‚úîÔ∏è Correct estimate</td> <td>‚ùå SHAP/feature importance may be misleading</td> </tr> <tr> <td><strong>Counterfactual</strong></td> <td>‚ÄúWhat would $Y$ have been if $X$ had been different?‚Äù</td> <td>‚Äú$Y=3$ when $X_3=$yellow. What if $X_3$=green?‚Äù</td> <td>Individual-level alternative-world reasoning</td> <td>‚úîÔ∏è Correct estimate</td> <td>‚ùå Generally impossible</td> </tr> <tr> <td><strong>Root Cause</strong></td> <td>Identify cause of an event</td> <td>‚ÄúWhat caused the failure?‚Äù</td> <td>Find initial causal driver</td> <td>‚úîÔ∏è Correct estimate</td> <td>‚ùå Possibly wrong</td> </tr> </tbody> </table> <h2 id="the-common-cause-principle">The Common Cause Principle</h2> <p>But how can observational data mislead us? Reichenbach‚Äôs <b>common cause principle</b><d-cite key="reichenbach1956direction"></d-cite> states:</p> <blockquote> <p>üí° If X and Y are correlated, then either X causes Y, Y causes X, or a hidden confounder causes both.</p> </blockquote> <p>Observational data cannot tell these apart. This limitation explains famous phenomena like <b>Simpson‚Äôs paradox</b><d-cite key="simpson1951interpretation"></d-cite>, where aggregated correlations reverse once you account for confounders. It also explains why <b>SHAP values and feature importance</b>, though useful, are not causal measures. They reflect importance <b>within the model</b>, not influence <b>in the real world</b>.</p> <h2 id="randomized-experiments-the-gold-standard-for-causality">Randomized Experiments: The Gold Standard for Causality</h2> <p>Given the limitations of observational data, a natural question arises: <i>how can causal effects be measured correctly?</i> Since the seminal work of Ronald Fisher<d-cite key="fisher1935design"></d-cite>, the gold standard for causal inference has been <b>randomized experimentation</b>, and in particular <b>Randomized Controlled Trials (RCTs)</b>.</p> <p>Randomized experiments aim to isolate causal effects by deliberately intervening on one or more variables of interest while holding all other factors constant <b>in expectation</b>. This is typically achieved through <b>random assignment</b>, which ensures that both observed and unobserved covariates are, on average, balanced across experimental groups. As a result, randomization removes confounding <i>by design</i>, allowing causal effects to be identified without relying on strong modeling assumptions.</p> <div class="row justify-content-sm-center"> <div class="col-sm-auto text-center mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ab-testing-480.webp 480w,/assets/img/ab-testing-800.webp 800w,/assets/img/ab-testing-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ab-testing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2" style="text-align: center;">An A/B test on an online platforms is an instance of a randomized controlled trial (RCT). Users are randomly assigned to interact with one of <i>two</i> webpage versions, for example, a new interface (left) or the existing <i>control</i> version (right). Outcomes of interest (e.g., user engagement) are measured over a fixed time period. Statistical analysis is then used to determine whether the introduced interface has a statistically significant causal effect on the outcome variables (e.g., exposure rates or sales). Illustration courtesy of Abstraktmg<d-footnote>https://www.abstraktmg.com/a-b-testing-in-marketing/ (last accessed: 30 December 2025).</d-footnote>. </div> </div> </div> <p>Common instances of randomized experimentation include:</p> <ul> <li><b>Randomized Controlled Trials (RCTs)</b> in medicine and the social sciences,</li> <li><b>A/B tests</b> in online platforms and digital systems,</li> <li><b>Controlled laboratory experiments</b> in the natural sciences,</li> <li><b>Simulation-based interventions</b> in synthetic or virtual environments.</li> </ul> <p>To illustrate, consider a clinical trial investigating whether a newly introduced treatment (let‚Äôs call it $X$) reduces the severity of migraines. An RCT proceeds by randomly assigning patients to either a <b>treatment group</b> (receiving treatment X) or a <b>control group</b> (receiving a placebo), and subsequently comparing outcomes across the two groups. Because assignment is random, any systematic difference in outcomes can be attributed to the intervention itself rather than to pre-existing differences among patients.</p> <p>A standard causal quantity estimated in such settings is the <b>Average Treatment Effect (ATE)</b>, defined as $\text{ATE} = \mathbb{E}[Y_1 - Y_0]$ where $Y_1$ and $Y_0$ denote the potential outcomes under treatment and control, respectively. If the ATE is statistically distinguishable from zero, one concludes that the intervention has a causal effect on the outcome of interest.</p> <p>The conceptual strength of RCTs lies in their <b>ability to eliminate confounding bias through randomization</b>, making them the most reliable tool for causal inference. However, despite their methodological appeal, randomized experiments are often <b>impractical or infeasible</b> in real-world settings. They can be prohibitively expensive, logistically complex, or ethically unacceptable, for instance, when studying the causal effects of harmful behaviors such as smoking. Moreover, in modern large-scale systems, the space of possible interventions is often combinatorial, rendering exhaustive experimentation impossible in practice.</p> <p>These limitations motivate the development of <b>model-based causal frameworks</b> that allow causal effects to be inferred without relying exclusively on randomized experiments. In the following sections, we introduce such a framework through structural causal models, which enable principled reasoning about interventions, counterfactuals, and distributional changes beyond purely observational data.</p> <h2 id="the-language-of-causality---scms--pearls-ladder">The Language of Causality - SCMs &amp; Pearl‚Äôs Ladder</h2> <p>The formal language in Causality to reason about causal mechanisms, interventions and counterfactuals are <b>Structural causal models (SCMs)</b>, introduced by Judea Pearl <d-cite key="pearl2009causality"></d-cite>. An SCM consists of:</p> <ol> <li><b>A Directed Acyclic Graph (DAG)</b> where nodes represent the examined variables of interest and direct arrows <b>direct causal effects</b> (e.g., Sun ‚Üí IceCream).</li> <li><b>Structural equations</b> ($X_i = f_i(\mathrm{Parents}(X_i), \epsilon_{X_i})$), representing how each variable $X_i$ is generated.</li> <li><b>Independent noise terms</b> $\epsilon_{X_i}$ for each variable $X_i$, representing unobserved and inherent randomness of the system.</li> </ol> <p>This framework allows us to compute three fundamentally different types of queries:</p> <table> <thead> <tr> <th>Type</th> <th>Question</th> <th>Example</th> </tr> </thead> <tbody> <tr> <td><b>Observational</b></td> <td>What do we see?</td> <td>$P(Y \mid X=x)$</td> </tr> <tr> <td><b>Interventional</b></td> <td>What happens if we <i>force</i> X to a value?</td> <td>$P(Y \mid do(X=x))$</td> </tr> <tr> <td><b>Counterfactual</b></td> <td>What would have happened <i>otherwise</i>?</td> <td>$P(Y_{x‚Äô} \mid X=x,Y=y)$</td> </tr> </tbody> </table> <aside><p>These three different types of queries are known as <b>Pearl's Ladder of Causal Hierarchy</b> <d-cite key="pearl2018book"></d-cite>, where predictive models can only answer queries of level 0, with causal models additionally to level 1 and 2.</p> </aside> <p>Predictive models only answer the first type, while causality accounts for all three.</p> <p>SCMs allow us to simulate interventions mathematically without performing physical experiments. Using <b>do-calculus</b>, we can compute $P(Y \mid do(X=x))$ which differs from the purely observational $P(Y \mid X=x)$ except in special, unconfounded cases (outside the scope of this blog). This gives us a way to predict interventions, <i>if we know the causal graph</i>.</p> <h2 id="how-do-we-discover-causal-graphs">How Do We Discover Causal Graphs?</h2> <p>This is where the field of <b>causal discovery</b> enters. Causal discovery attempts to learn the structure of the system (the DAG) from data (often observational, sometimes including interventional samples), which operate under certain causal assumptions. For example, the causal sufficiency assumption assumes no hidden confounders, while faithfulness (loosely defined in the scope of this blog post) that no determinism exists in the examined system, hence no causality (e.g. variables that are deterministically related like ratios of variables). Once the graph is known, <b>causal inference</b> methods estimate effects such as:</p> <ul> <li>ATEs</li> <li>mediation effects</li> <li>optimal intervention strategies</li> <li>counterfactual outcomes</li> </ul> <p>In practice, as the ground truth causal model is unknown, one first discovers the causal structure (the causal DAG and the SCM representation) and then estimates causal effects.</p> <h2 id="an-illustrative-example-the-provided-examples-are-available-in-a-curated-github-repository-httpsgithubcomkougiouliscausality-tutorial-with-various-causality-tutorials-for-the-motivated-reader">An Illustrative Example <d-footnote>The provided examples are available in a curated Github Repository (https://github.com/kougioulis/causality-tutorial) with various causality tutorials for the motivated reader.</d-footnote></h2> <p>Consider 3 variables, $X,Y,Z$. $X$ and $Z$ are observed to have correlation coefficient $3$. That is, if you observe $1$ unit of change in $X$, you observe $3$ units of change in $Z$. Let‚Äôs try to answer the following questions:</p> <aside> <p>For simplicity purposes, we assume linear relationships.</p> </aside> <ul> <li>‚ùì If you intervene and change $X$ (not merely observe) in the real world, what change would you observe?</li> <li>‚ùì If you build a predictive model from $X$ to $Z$ and change the input to the model, would you observe the same change when you intervene in the real world?</li> </ul> <p>For linear correlations and causal relations with Gaussian additive noise, we make the following assumptions:</p> <ul> <li>‚òùÔ∏è Correlations on a path <b>multiply</b> together</li> <li>‚òùÔ∏è Correlations from different path <b>sum</b></li> </ul> <p>We begin by importing some needed modules:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div> <p>Consider a <b>causal</b> DAG, assuming linear relationships with edges $X \leftarrow Y \rightarrow Z$ and $X \rightarrow Z$. A directed edge illustrates direct causal influence (i.e. $X$ directly causes $Y$, $X$ is the direct cause of $Y$, $Y$ is the direct effect of $X$) while indirect causal relationships are illustrated by directed paths. The linear coefficients of each causal edge are $0.7$ for $Y \rightarrow X$, $-8$ for $Y \rightarrow Z$ and $13$ for $X \rightarrow Z$. Loosely defined, pairing this causal DAG with the functional dependencies of its variable given its parents (direct causes), plus an additive noise term (accounting for randomness), creates an (additive) structural causal model (SCM). Let‚Äôs try to answer the following question:</p> <blockquote>What is going to happen when I increase X by 1?</blockquote> <ul> <li>The non-causal path $X \leftarrow Y \rightarrow Z$ has coefficient $0.7 \cdot (-8)=-5.6$.</li> <li>The path $X -&gt; Z$ coefficient $13$ causal effect causal path.</li> <li>The total correlation coefficient observed is $0.7 - 8 = -7.3$.</li> </ul> <p>We observe that the <em>causal effect is larger than the computed correlation!</em> (Causal effect is positive, observed correlation is negative)</p> <p>We define the SCM of the three variables:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Defining structural equations of the SCM
# X &lt;- Y (coef 5) + noise
# Z &lt;- Y (coef -2) + X (coef 13) + noise
</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">6</span>  <span class="c1"># large enough sample size
</span>
<span class="c1"># Exogenous Gaussian noise
</span><span class="n">eps_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">eps_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">eps_z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

<span class="c1"># Structural Causal Model
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">eps_y</span>
<span class="n">X</span> <span class="o">=</span> <span class="mf">0.7</span> <span class="o">*</span> <span class="n">Y</span> <span class="o">+</span> <span class="n">eps_x</span>
<span class="n">Z</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span> <span class="o">*</span> <span class="n">Y</span> <span class="o">+</span> <span class="mi">13</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">eps_z</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">:</span> <span class="n">Y</span><span class="p">,</span> <span class="sh">"</span><span class="s">Z</span><span class="sh">"</span><span class="p">:</span> <span class="n">Z</span><span class="p">})</span>
</code></pre></div></div> <p>And compute the observed Pearson correlation between X and Z, as well as the predictive effect (regression $Z \sim X$):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pearson Correlation between X and Z (observational / predictive)
</span><span class="n">corr_xz</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">corrcoef</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Predictive model: Z ~ X by ordinary least squares regression
</span><span class="n">X_df</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nf">add_constant</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">])</span>
<span class="n">model_pred</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nc">OLS</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">Z</span><span class="sh">"</span><span class="p">],</span> <span class="n">X_df</span><span class="p">).</span><span class="nf">fit</span><span class="p">()</span>
<span class="n">pred_effect</span> <span class="o">=</span> <span class="n">model_pred</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Observed correlation between X and Z:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">corr_xz</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="c1"># 0.862
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Predictive effect (regression Z ~ X):</span><span class="sh">"</span><span class="p">,</span> <span class="nf">round</span><span class="p">(</span><span class="n">pred_effect</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="c1"># 9.246
</span></code></pre></div></div> <p>From the above snippet, we obtain the following contradictory results:</p> <ul> <li> <p><b>Causal model:</b> Increase $X$ by $1$, increase Z by $0.86$.</p> </li> <li> <p><b>Predictive model:</b> Increase $X$ by $1$, increase Z by $9.25$.</p> </li> </ul> <p>The correct approach, would instead be to:</p> <ul> <li>1Ô∏è‚É£ Learn the causal model (via a <b>causal discovery</b> algorithm, in this example we assume it is known correctly.)</li> <li>2Ô∏è‚É£ Identify the <b>non-causal paths</b> and remove the effect of the non-causal paths and only (identify the quantities that block the correlations from non-causal paths, called the <b>adjustment set</b>).</li> </ul> <aside> <p><b>Adjustment set:</b>a set of variables that, when conditioned on, blocks all backdoor (confounding) paths between a treatment and an outcome, enabling unbiased causal effect estimation from observational data.</p> </aside> <ul> <li>3Ô∏è‚É£ Build a <b>predictive model</b> that <b>includes an adjustment set</b> and only and hence controls for their values.</li> </ul> <p>The correlation coefficient of $X$ to $Z$ conditioned on fixed values of $Y$ provide the <b>true causal effect</b> of $13$ units with the following code snippet, with an absolute error in the third decimal place.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># True causal effect (by an intervention do(X))
</span><span class="n">causal_effect</span> <span class="o">=</span> <span class="mi">13</span>  <span class="c1"># known from our structural equations
</span>
<span class="c1"># Adjustment set approach (control for Y in regression)
</span><span class="n">XZ_df</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nf">add_constant</span><span class="p">(</span><span class="n">data</span><span class="p">[[</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Y</span><span class="sh">"</span><span class="p">]])</span>
<span class="n">model_adj</span> <span class="o">=</span> <span class="n">sm</span><span class="p">.</span><span class="nc">OLS</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">Z</span><span class="sh">"</span><span class="p">],</span> <span class="n">XZ_df</span><span class="p">).</span><span class="nf">fit</span><span class="p">()</span>
<span class="n">adj_effect</span> <span class="o">=</span> <span class="n">model_adj</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">"</span><span class="s">X</span><span class="sh">"</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Causal effect (true do-intervention): </span><span class="si">{</span><span class="n">causal_effect</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># 13
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Causal effect via adjustment set (regression Z ~ X + Y): </span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="n">adj_effect</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># 13.002
</span></code></pre></div></div> <div class="row justify-content-sm-center"> <div class="col-sm-auto text-center mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/interv_obs-480.webp 480w,/assets/img/interv_obs-800.webp 800w,/assets/img/interv_obs-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/interv_obs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2" style="text-align: center;">Histogram of the conditional observational distribution $Z|X=1$ against the interventional distribution $Z|\text{do}(X=1)$ further illustrates the difference between the two (and convince us that in general, $P(Z | X=x) \neq P(Z | do(X=x))$).</div> </div> </div> <p>As shown in the histogram, the two distributions $Z \mid X \approx 1$ and $Z \mid do(X = 1)$ differ dramatically because:</p> <ul> <li> <p>Under <b>conditioning</b>, when we observe $X = 1$, that typically means <b>Y was high</b>, because $X = 0.7Y + \text{noise}$. Thus the backdoor path $X \leftarrow Y \rightarrow Z$ pushes Z down via the $-8Y$ term.</p> </li> <li> <p>Under <b>intervention</b>, when we force $X = 1$, $Y$ is no longer correlated, so only the direct causal effect $(13X)$ applies.</p> </li> </ul> <h2 id="back-to-the-ice-cream-sales-example">Back to the Ice Cream Sales Example</h2> <p>Let‚Äôs return to the ice cream example illustrated at the beginning of the post, but this time with some code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">samples</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># observational samples (iid)
</span>
<span class="c1"># True causal mechanism: ice_cream sales &lt;- sun -&gt; sunburn
</span><span class="n">sun</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="p">)</span>
<span class="n">ice_cream</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sun</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="p">)</span>
<span class="n">sunburn</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">sun</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Predictive model incorrectly learns sunburn -&gt; ice cream
</span><span class="n">X</span> <span class="o">=</span> <span class="n">sunburn</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">pred_model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ice_cream</span><span class="p">)</span>

<span class="c1"># Intervene: sunscreen campaign sets sunburn to a constant
</span><span class="n">sunburn_intervened</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Predictive model's WRONG prediction under intervention
</span><span class="n">predicted_icecream</span> <span class="o">=</span> <span class="n">pred_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">sunburn_intervened</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># True causal outcome under intervention - ice cream depends ONLY on sunlight, not sunburn (graph surgery is performed on the underlying SCM)
</span><span class="n">true_icecream</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sun</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">samples</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">True Ice Cream Sales (causal model): </span><span class="si">{</span><span class="n">true_icecream</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> units</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># 10.04 units
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Predicted Ice Cream Sales (predictive model): </span><span class="si">{</span><span class="n">predicted_icecream</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> units</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># 2.05 units 
</span></code></pre></div></div> <p>A predictive model wrongly suggests that an increase in one unit of radiation will account for two units of increase in ice cream sales, which is 5 times less that the true causal effect which we obtained using the true causal model.</p> <p>As we have noticed already, a predictive model is doomed to failure if applied to cases outside the trained distribution: For example, consider applying the above simple regression model on a hypothetical subpopulation of Scandinavians (who are less exposed to sunlight compared to other regions) who enjoy eating ice cream all year long.</p> <h2 id="closing-thoughts">Closing Thoughts</h2> <p>Predictive models excel at finding patterns in data, but patterns alone are not enough when decisions, interventions, or changing environments are involved. Predictive models (either from classical ML up to deep learning approaches) have found great sucess, yet they prove unstable when the underlying perturbed.</p> <div style="max-width: 450px; margin: 0 auto; zoomable:true"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/causality_meme-480.webp 480w,/assets/img/causality_meme-800.webp 800w,/assets/img/causality_meme-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/causality_meme.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Causal modeling addresses this gap by explicitly representing <b>how</b> data is generated. By reasoning about interventions and counterfactuals, causal methods allow models to generalize beyond the conditions under which they were trained and to support meaningful actions.</p> <p>It is to no surpise that decades after Pearl‚Äôs formulation of causality, the industry is just starting to adopt causal discovery and causal inference methods for optimized decision making and creation of <i>causal digital twins</i>, especially in the case of time-series data (by Temporal Structural Equation Models - TSCMs<d-cite key="runge2018causal"></d-cite><d-cite key="runge2019inferring"></d-cite>), now called <b>Causal AI</b> to contrast approaches using traditional ML or deep learning and LLMs.</p> <aside><p> A causal digital twin is a causal model of an examined system that combines a real-time state representation alongside a causal model (e.g., a structural causal model), allowing counterfactual reasoning, intervention analysis, and policy evaluation. </p></aside> <p>What‚Äôs your view? Let me know in the comments! üöÄ</p>]]></content><author><name></name></author><category term="Comments"/><category term="causality,"/><category term="causal-ML,"/><category term="predictive-models"/><summary type="html"><![CDATA[And what your ML models are missing]]></summary></entry><entry><title type="html">Glove80: Keyboard Endgame?</title><link href="https://nkougioulis.com/blog/2025/glove80/" rel="alternate" type="text/html" title="Glove80: Keyboard Endgame?"/><published>2025-06-05T08:00:00+00:00</published><updated>2025-06-05T08:00:00+00:00</updated><id>https://nkougioulis.com/blog/2025/glove80</id><content type="html" xml:base="https://nkougioulis.com/blog/2025/glove80/"><![CDATA[<h3 id="introduction">Introduction</h3> <div class="row justify-content-sm-center"> <div class="col-sm-auto text-center mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Glove80-main-img-768x432-480.webp 480w,/assets/img/Glove80-main-img-768x432-800.webp 800w,/assets/img/Glove80-main-img-768x432-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Glove80-main-img-768x432.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Glove80 Typing Illustration" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2" style="text-align: center;">Glove80 in action, in white color and palm rests. Taken explicitely from <a href="https://develop3d.com/product-design/fits-like-a-glove80-designed-for-keyboard-comfort/">Develop3D</a>.</div> </div> </div> <p>There comes a point in life where you need to take a step back from your average keyboard and opt for a more ergonomic choice. Personally, this rite of passage came after working long hours; software engineering and writing consist of a lot of keystrokes. My fingers were begging me for a better typing alternative. Now would you consider yourself lucky or unlucky in this case? Apart from the weird looks you may get at the office due to the abnormal look of your typing gear, there are a plethora of advantages in using ergonomic keyboards, such as reduced wrist pain, tendon strain and reduced chances for RSI and Carpal Tunnel Syndrome. I would even add they prevent hunching over the keyboard like a monkey. Although a considerable disadvantage is that they come with a hefty price tag, since (to my surpris) they represent a niche market, I‚Äôd assume that initial investment would cost less than any future visits to any doctor and rehabilitation specialist. The pioneering brand <a href="https://kinesis-ergo.com/">Kinesisgg</a>, which was the first to introduce contoured keyboards back in 1992, offers a <a href="https://kinesis-ergo.com/split-keyboards/">lengthy post on reasons to choose a split ergonomic keyboard</a>. In their view‚Äîand now mine‚Äîtraditional keyboards have several ergonomic flaws, such as:</p> <ul> <li><b>Ulnar deviation</b>: This occurs when wrists bend outward (towards the side of the pinky ) to reach the home row on a standard keyboard. Since the keyboard is a single flat slab, the hands have to angle inward unnaturally to reach the keys, placing strain on your wrists and forearms.</li> <li><b>Forearm pronation</b>: Traditional keyboards force your palms to rest flat on the desk, with your forearms rotated inward (palms down). This posture twists the forearm bones and tenses up the muscles, reducing blood flow and increasing fatigue.</li> <li><b>Wrist extension</b>: Most conventional keyboards are higher in the back, which causes the wrists to bend upward when typing. This ‚Äúextension‚Äù posture compresses the carpal tunnel and adds tension to the tendons‚Äîespecially bad for long sessions.</li> <li><b>Mouse overreach</b>: Because the keyboard is a single block, your right hand has to reach around or over the number pad to get to the mouse. This leads to unnecessary shoulder and arm strain over time.</li> </ul> <p>In a nutshell, modern ergonomic keyboards eliminate the above problems by spliting the keyboard into two halves, one left and one right. This way, not only is the user able to type with their hands on shoulder width, but also encourages learning correct typing practices. For instance, many people overreach keys with the wrong finger‚Äîlike pressing the ‚ÄúU‚Äù with the left index finger instead of the right. I discovered several of my own bad typing habits this way‚Äîhabits that had become deeply ingrained after years of using traditional keyboards. The mouse is most usually placed in the middle of the two halves.</p> <div class="row justify-content-sm-center"> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ulnar-deviation-problem-300x219-480.webp 480w,/assets/img/ulnar-deviation-problem-300x219-800.webp 800w,/assets/img/ulnar-deviation-problem-300x219-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ulnar-deviation-problem-300x219.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Ulnar deviation problem" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/forearm-pronation-problem-300x21-480.webp 480w,/assets/img/forearm-pronation-problem-300x21-800.webp 800w,/assets/img/forearm-pronation-problem-300x21-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/forearm-pronation-problem-300x21.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Forearm pronation problem" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wrist-extension-problem-300x219-480.webp 480w,/assets/img/wrist-extension-problem-300x219-800.webp 800w,/assets/img/wrist-extension-problem-300x219-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/wrist-extension-problem-300x219.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Wrist extension problem" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-3 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mouse-overreach-problem-300x219-480.webp 480w,/assets/img/mouse-overreach-problem-300x219-800.webp 800w,/assets/img/mouse-overreach-problem-300x219-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mouse-overreach-problem-300x219.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Mouse overreach problem" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Ergonomic risk factors of traditional keyboards: ulnar deviation, forearm pronation, wrist extension, and mouse overreach. Illustrations courtesy of <a href="https://kinesis-ergo.com/solutions/keyboard-risk-factors">Kinesis Corporation</a>.</div> </div> <p>Being based in Europe, choices are a bit limited. A plethora of starting boards are available from <a href="https://splitkb.com/collections/keyboard-kits">splitkb.com</a>, either pre-soldered or unsoldered (as a DIY kit). Being a complete rookie with split keyboards (and mechanical, ortholinear keyboards in general), I found all of them, including the highly popular <a href="https://www.boardsource.xyz/products/Corne">Corne</a> model, to have too few buttons for my liking (I want F keys, numbers and arrow buttons). As such, after weeks of roaming the internet forums, the final considerations compiled were: The range of products by Kinesis, especially the wireless <a href="https://kinesis-ergo.com/keyboards/advantage360/">Advantage 360</a>, the <a href="https://www.moergo.com/">Glove80</a> from MoErgo and the <a href="https://dygma.com/pages/defy">Dygma Defy</a>. Both the Advantage 360 and the Defy turned out to be considerably more expensive once features like RGB underglow, tenting, Bluetooth connectivity and palmrests were added (all of which come standard to the Glove80), let alone import duties. After reading a wave of glowing reviews, the final choice was, frankly, a no-brainer. One a side one, I ended up paying no import duties, although shipped from China, because apparently keyboards have 0% import duties.</p> <div class="row justify-content-sm-center"> <div class="col-sm-auto text-center mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MoErgoCherryBlossom-480.webp 480w,/assets/img/MoErgoCherryBlossom-800.webp 800w,/assets/img/MoErgoCherryBlossom-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/MoErgoCherryBlossom.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="MoErgo Cherry Blossom Illustration" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2" style="text-align: center;">The company also offers a <a href="https://www.moergo.com/pages/glove80-key-switches">variety of key switch choices</a>. I opted for the newly introduced exclusive Cherry Blossom üå∏ 30gf silent switches, based on the Kailh Choc switch. The latter are also paired with clear cases for better RGB when turned on and is ideal for use in communal places such as the office or the library. As I found out, they are indeed barely audible and typing on them feels crazy comfortable. Two colors for the body are available, white and black (although they refer to it as grey). I opted for the cleaner white look, matching the keycaps.</div> </div> </div> <h3 id="features-of-the-glove80">Features of the Glove80</h3> <div class="row justify-content-sm-center"> <div class="col-sm-auto text-center mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/F4QCWBuagAAKK8O-480.webp 480w,/assets/img/F4QCWBuagAAKK8O-800.webp 800w,/assets/img/F4QCWBuagAAKK8O-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/F4QCWBuagAAKK8O.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Glove80 Travel Case" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2" style="text-align: center;">The MoErgo travel case, taken from MoErgo's Twitter post. Mine shipped with two pouches instead of the shown cylinder cases.</div> </div> </div> <p>The Glove80 comes packed with an impressive list of features that cater to both ergonomic purists and mechanical keyboard enthusiasts:</p> <ul> <li>Fully mechanical, ortholinear design (switches can be lubed with a thin-viscosity oil for even smoother feel)</li> <li>Wireless full-split layout</li> <li>Concave keywells for natural finger positioning</li> <li>6-key, 2-row curved thumb clusters</li> <li>Adjustable tenting with magnetic feet</li> <li>USB-C and Bluetooth connectivity</li> <li>Detachable palm rests</li> <li>80 keys with hot-swappable keycaps</li> </ul> <p>I personally opted to not using the palm rests, although prominment in other ergonomic keyboards too like the Kinesis Advantage 360, as I prefer my wrists resting lower, using a high tenting angle. The keyboard also ships with a travel case ‚Äî though frankly, it‚Äôs a bit bulky for carrying around, although the creators have given a lot of attention to designing it, as explained in <a href="https://www.reddit.com/r/ErgoMechKeyboards/comments/16yn5hg/ad_lessons_learned_from_designing_a_travel_case/">this Reddit post</a>. I find myself wrapping the halves in a cloth and tossing them in my backpack instead. Fortunately, they‚Äôre lighter than I expected, which makes them easy to carry around. That said, I wouldn‚Äôt mind a bit more weight for extra stability during desk use.</p> <div class="row justify-content-sm-center"> <div class="col-sm-auto text-center mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/White-no-palm-rest-angle-1200x1200-480.webp 480w,/assets/img/White-no-palm-rest-angle-1200x1200-800.webp 800w,/assets/img/White-no-palm-rest-angle-1200x1200-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/White-no-palm-rest-angle-1200x1200.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="MoErgo Glove80" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2" style="text-align: center;">The Glove80 keyboard (white version), pictured without the detachable palm rests and with the Kailh Choc switches. Courtesy of MoErgo.</div> </div> </div> <h3 id="initial-impressions">Initial Impressions</h3> <p>The moment I first tried typing on the Glove80, I was humbled ‚Äî barely managing 9 WPM on <a href="https://monkeytype.com">monkeytype.com</a>. That instant made me question whether investing in such a niche piece of gear was worth it or if I had just tossed my money away. The learning curve is definitely steep if you jump in without guidance or the right approach. What helped me tremendously was using <a href="https://keybr.com">keybr.com</a>, a website that lets you practice individual letters, then letter combinations, making the learning process structured and manageable. After spending several afternoons practicing over the course of a week, my typing speed bounced back quickly. From around 29W PM in Day 2, typing speed managed to consistently get around 80 WPM 8 days after.</p> <p>Within about a month, I was hitting close to 100 WPM, and after roughly three months, I topped out around 115 WPM.</p> <p>Bluetooth connectivity is awesome, with multiple channels supported, though I occasionally have to restart the two halves due to minor latency hiccups. Battery life is great ‚Äî from my experience, it lasts roughly a month of daily use without RGB underglow. RGB lighting definitely drains the battery faster. The left half needs charging more often since it acts as the ‚Äúbrain‚Äù of the keyboard, managing the Bluetooth connection and communication with the right half. The transparent casings of the switches really help the RGBs shine through, and you get a plethora of colors to choose from for the underglow, as well as different degrees of illumination.</p> <p>The package also comes with a generous accessory kit, including 6 white-labeled keycaps, USB-C cable for charging, a keycap puller, twelve extra feet for high-angle and custom mounting, a set of extra threaded rods and nuts for high-angle mounting (my opinion on this in a moment), an M4 spanner and lastly four extra blank MCC keycaps and two blank MBK keycaps to serve as homing keys.</p> <div class="row justify-content-sm-center"> <div class="col-sm-auto text-center mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/IMG_20250115_121635_blurred-480.webp 480w,/assets/img/IMG_20250115_121635_blurred-800.webp 800w,/assets/img/IMG_20250115_121635_blurred-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/IMG_20250115_121635_blurred.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="The Glove80 in action at my office desk in Heraklion, with tenting, paired with the MX Vertical mouse by Logitech in the middle of the two halves." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption mt-2" style="text-align: center;">The Glove80 in action at my office desk in Heraklion, with tenting, paired with the MX Vertical mouse by Logitech in the middle of the two halves.</div> </div> </div> <h3 id="pros">Pros</h3> <ul> <li> <p><b>Keycaps and Typing Feel</b>: The keycaps feel absolutely wonderful‚Äîlike typing on a cloud. This soft, tactile experience is unlike any other keyboard I‚Äôve used. The concave keywells perfectly cradle the fingers, enhancing comfort and accuracy.</p> </li> <li> <p><b>Thumb Cluster</b>: The 6-key, 2-row curved thumb cluster is a joy to use. All keys are within easy reach, significantly improving typing ergonomics and speed.</p> </li> <li> <p><b>Wireless and Connectivity</b>: Bluetooth and USB-C options work reliably, with impressive battery life‚Äîabout a month of daily use without RGB lighting.</p> </li> <li> <p><b>Customizability</b>: Detachable palm rests, adjustable tenting, and a variety of key switches offer a level of customization rarely found at this price point.</p> </li> <li> <p><b>Extras</b>: The package includes handy extras like a keycap puller, extra feet for high-angle mounting, O-rings, and replacement keycaps - small but thoughtful touches, all in two smooth grey pouches.</p> </li> </ul> <h3 id="cons">Cons</h3> <p>Some cons that I found over this 6 month usage period are:</p> <ul> <li> <p><b>Travel Case</b>: The included travel case is bulky and not really suitable for light travel. Unlike the clever foam-molded Dygma Defy case, the Glove80‚Äôs case feels oversized. I usually just wrap the keyboards in a cloth and toss them in my backpack instead. The case mainly stays home unless I‚Äôm traveling with carry-on luggage.</p> </li> <li> <p><b>Build Quality Issues</b>: The plastic body (excluding their lovely custom keycaps) feels somewhat cheap. There are noticeable mold inconsistencies, especially around the left thumb cluster‚Äôs outer edge. For a premium-priced keyboard, better finishing and material quality would be expected.</p> </li> <li> <p><b>Tenting Mechanism</b>: Although adjustable, the tenting system is rather basic‚Äîjust three screws that can loosen over time, requiring frequent readjustment. The suggested fix involving nuts doesn‚Äôt feel like a well-thought-out solution for long-term stability.</p> </li> <li> <p><b>Non-Swappable Switches</b>: Some users prefer hot-swappable switches to easily try different feels or replace faulty switches. The Glove80‚Äôs switches are soldered, limiting this flexibility.</p> </li> </ul> <h3 id="closing-thoughts">Closing Thoughts</h3> <p> The Glove80 is a lovely piece of tech, and although not experienced with other ergonomic options, I can understand why it is regarding as a highly comfortable keyboard. While it has a few quirks‚Äîlike the bulky travel case and some build quality issues‚Äîthe overall comfort, key feel, and thoughtful ergonomic design really outweigh these minor flaws. I hope the founders of MoErgo (the company behind the Glove80) address these issues in the next versions and as the product matures. Paired with my <a href="https://www.logitech.com/en-us/products/mice/mx-vertical-ergonomic-mouse.910-005447.html" target="_blank" rel="noopener noreferrer">Logitech MX Vertical</a> ergonomic mouse, positioned comfortably between the two halves, I feel like saying goodbye to any discomfort. The learning curve is definitely a bit steep but in the end rewarding, as the typing experience, comfort and efficiency cannot really be compared to the average keyboard.</p>]]></content><author><name></name></author><category term="glove80,"/><category term="split-keyboard,"/><category term="review"/><summary type="html"><![CDATA[It truly fits like a glove.]]></summary></entry><entry><title type="html">Gradient Wars - Reparametrization vs REINFORCE</title><link href="https://nkougioulis.com/blog/2025/gradient-estimation/" rel="alternate" type="text/html" title="Gradient Wars - Reparametrization vs REINFORCE"/><published>2025-01-03T09:12:00+00:00</published><updated>2025-01-03T09:12:00+00:00</updated><id>https://nkougioulis.com/blog/2025/gradient-estimation</id><content type="html" xml:base="https://nkougioulis.com/blog/2025/gradient-estimation/"><![CDATA[<p>Happy new year üéâüéáüéÜ! I was cleaning the dust out of some past handwritten notes on REINFORCE, when I decided to write up this short post, comparing the method against the known reparametrization trick, most often seen in training Variational Autoencoders.</p> <p>Variational Autoencoders (VAEs), introduced by Kingma and Welling <d-cite key="kingma2013auto"></d-cite>, are a class of deep latent variable models consisting of two coupled, independently parametrized components: an encoder(recognition model) and a decoder (generative model). Without going out of scope with details, the true posterior distribution over the latent variables is intractable to compute directly, so a variational approximation is used instead. A concrete example would be a prior \(p(z)\) as an isotropic Gaussian distribution and the stochastic encoder is modeled as a multivariate normal \(\mathcal{N}(\mu_\theta(z), \text{diag}(\sigma_\phi(z)))\), where \(\mu_\theta(z)\) and \(\sigma_\phi(z)\) are neural networks. The decoder is similarly structured with a neural network mapping the latent variable \(z\) back to the data space, thus enabling efficient learning of both the generative process and the inference model.</p> <p>As an image is a thousand words, I directly borrow an <a href="https://medium.com/@elzettevanrensburg/generating-the-intuition-behind-variational-auto-encoders-vaes-c7d2f8631a87">excellent illustration</a> of a Probabilistic Autoencoder by Elzette van Rensburg, as discussed before.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vae-illustration.webp" sizes="95vw"/> <img src="/assets/img/vae-illustration.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="the-problem">The Problem</h2> <p>Consider a random variable \(z \sim p_\theta (z)\), where \(p_\theta\) is a parametric distribution and a function \(f\), for which we wish to compute the gradient of its expected value. For instance, \(f\) may express a cost function such as the likelihood. If our aim is to minimize the likelihood \(L(\theta) = \mathbb{E}_{z \sim p_{\theta}(z)}[f(z)]\), or more formally \(\min \left\{ L(\theta) = \mathbb{E}_{Z \sim p_{\theta (z)}} [f(z)] \right\}_\theta\), then we are interested in evaluating (or at least, estimating) the gradient</p> \[\nabla_\theta \mathbb{E}_{z \sim p_{\theta}(z)}[f(z)]\] <p>where the variable notation \(z\) is used to refer to computing the expected value from a variable in the latent space. The issue here lies in computing the expectation with respect to the parameter distribution \(p(z)\), making it inherently stochastic instead of deterministic. That‚Äôs where path-derivative gradient estimators come to the rescue.</p> <h2 id="the-reparametrization-trick">The Reparametrization trick</h2> <p>The reparametrization trick can be briefly expressed as follows: If the distribution is reparameterizable, then \(z = g(\theta, \epsilon)\), where \(g\) is a deterministic function of parameters \(\theta\) and \(\epsilon\) is an independent random variable (noise). This leads to:</p> \[\frac{\partial}{\partial \theta} \mathbb{E}_{z \sim p_\theta(z)}[f(z)] = \frac{\partial}{\partial \theta} \mathbb{E}_\epsilon [f(g(\theta, \epsilon))]\] <p>Which simplifies to:</p> \[\mathbb{E}_{\epsilon \sim p_\epsilon} \left[ \frac{\partial f}{\partial g} \frac{\partial g}{\partial \theta} \right]\] <p>Thus, \(z\) is reparameterized as a function of \(\epsilon\), and the stochasticity of \(p_\theta\) is pushed to the distribution \(q(\epsilon)\), where \(q\) can be chosen as any random noise distribution. For example, if \(Z \sim N(\mu, \sigma^2)\), then \(z = \mu + \sigma \cdot N(0, 1)\). Another example would be, for a uniformly distributed variable \(U \sim U(a,b)\), that \(u = a + (b - a) \cdot U(0, 1)\).</p> <p>This bright trick alters the expectation to a distribution independent of \(\theta\), which can now be computed using Monte Carlo estimation, provided that \(f(g_\theta(\epsilon))\) is differentiable with respect to \(\theta\). Assuming a batch sample of size \(N\), the gradient estimator is</p> \[\nabla_\theta \mathbb{E}_{x \sim p(x)}[f(x)] \approx \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta f(g_\theta(\epsilon_i))\] <p>The reparametrization trick belongs to the family of Path-derivative gradient estimators. Path-derivative estimators focus on how the expected value of a function changes as a parameter of the underlying distribution changes. This is done by considering a ‚Äúpath‚Äù through the parameter space. By expressing the sampled value as a deterministic function of a differentiable random variable, it creates a smooth and differentiable path for the sampling process.</p> <p>A really intuitive illustration of the above, borrowed from the excellent writing of Kingma and Welling <d-cite key="kingma2019introduction"></d-cite> is shown here:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/reparameterization-trick-480.webp 480w,/assets/img/reparameterization-trick-800.webp 800w,/assets/img/reparameterization-trick-1200.webp 1200w," type="image/webp" sizes="95vw"/> <img src="/assets/img/reparameterization-trick.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <h2 id="score-function-estimator---reinforce">Score function Estimator - REINFORCE</h2> <p>Another gradient estimation method is the <strong>score-function estimator (SF)</strong> (also known as <strong>likelihood-ratio</strong>, or more commonly, <strong>REINFORCE</strong>) <d-cite key="williams1992simple"></d-cite>, which exploits the log-derivative property. It relies on the score-function (gradient of the log-likelihood) to estimate the gradient, without expressing the random variable as a function of a differentiable random variable (as in path-derivative estimators).</p> <p>The differentiation rule of the logarithm is \(\nabla_\theta \log p_\theta(z)= \frac{\nabla_\theta p_\theta(z)}{p_\theta(z)} \Rightarrow \nabla_\theta p_\theta(z) = p_\theta(z) \nabla_\theta \log p_\theta(z)\), where \(\nabla_\theta \log p_\theta(z)\) is known as the score-function. Hence we can rewrite the gradient as</p> \[\nabla_\theta \mathbb{E}_Z [f(z)] = \mathbb{E}_Z \left[ f(z) \nabla_\theta \log p_\theta(z) \right]\] <p>REINFORCE only requires that \(p_\theta(z)\) is differentiable with respect to the parameters \(\theta\), and it does not require backpropagating through \(f\) at the sample \(z\). It must also be easy to sample from \(p_\theta(z)\). Just like before, for a sample batch of size \(N\), we approximate the expectation as</p> \[\mathbb{E}_Z \left[ f(z) \nabla_\theta \log p_\theta(z) \right] \approx \frac{1}{N} \sum_{i=1}^{N} f(x_i) \nabla_\theta \log p_\theta(x_i)\] <p>which is an <strong>unbiased estimator</strong> of the gradient (in plain terms, for infinite sample size the above expected value is the same as the true gradient). Although unbiased, known drawback of REINFORCE is high variance of the estimation, leading to slow training convergence for relatively small samples. Finally, it is worth noting that the variance of SF can be reduced by substracting a <strong>control variate</strong> \(b(z)\) as follows:</p> \[\begin{split} \nabla_\theta \mathbb{E}_Z [f(z)] &amp;= \mathbb{E}_Z [f(z) \nabla_\theta \log p_\theta(z)] \\ &amp;= \mathbb{E}_Z [f(z) \nabla_\theta \log p_\theta(z) + b(z) \nabla_\theta \log p_\theta (z) - b(z) \nabla_\theta \log p_\theta(z)] \\ &amp;= \mathbb{E}_Z [(f(z) - b(z)) \nabla_\theta \log p_\theta (z) ] + \mu_b \end{split}\] <p>where \(\mu_b \equiv \mathbb{E}_Z [b(z) \nabla_\theta \log p_\theta(z)]\). Known estimators with control variates are NVIL <d-cite key="mnih2014neural"></d-cite>, MuProp <d-cite key="gu2015muprop"></d-cite>(unbiased), VIMCO <d-cite key="mnih2016variational"></d-cite> among others.</p> <p>¬†</p>]]></content><author><name></name></author><category term="Comments"/><category term="VAEs"/><category term="gradient-estimation"/><category term="reparametrization-trick"/><category term="reinforce"/><summary type="html"><![CDATA[A ride through gradient estimators]]></summary></entry><entry><title type="html">First post</title><link href="https://nkougioulis.com/blog/2024/first-post/" rel="alternate" type="text/html" title="First post"/><published>2024-12-28T10:00:00+00:00</published><updated>2024-12-28T10:00:00+00:00</updated><id>https://nkougioulis.com/blog/2024/first-post</id><content type="html" xml:base="https://nkougioulis.com/blog/2024/first-post/"><![CDATA[<p>Just getting started. Personal thoughts and notes expected here üôÇ.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Just getting started. Personal thoughts and notes expected here üôÇ.]]></summary></entry></feed>